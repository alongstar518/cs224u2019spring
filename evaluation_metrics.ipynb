{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Overview](#Overview)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Classifier metrics](#Classifier-metrics)\n",
    "  0. [Confusion matrix](#Confusion-matrix)\n",
    "  0. [Accuracy](#Accuracy)\n",
    "  0. [Precision](#Precision)\n",
    "  0. [Recall](#Recall)\n",
    "  0. [F scores](#F-scores)\n",
    "  0. [Macro-averaged F scores](#Macro-averaged-F-scores)\n",
    "  0. [Weighted F scores](#Weighted-F-scores)\n",
    "  0. [Micro-averaged F scores](#Micro-averaged-F-scores)\n",
    "  0. [Precision–recall curves](#Precision–recall-curves)\n",
    "  0. [Average precision](#Average-precision)\n",
    "0. [Regression](#Regression)\n",
    "  0. [Mean squared error](#Mean-squared-error)\n",
    "  0. [R2 scores](#R2-scores)\n",
    "0. [Sequence prediction](#Sequence-prediction)\n",
    "  0. [Word error rate](#Word-error-rate)\n",
    "  0. [BLEU scores](#BLEU-scores)\n",
    "  0. [Perplexity](#Perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "1. Different evaluation metrics __encode different values__ and have __different biases and other weaknesses__. Thus, you should choose your metrics carefully, and motivate those choices when writing up and presenting one's work.\n",
    "\n",
    "1. This notebook reviews some of the most prominent evaluation metrics in NLP, seeking not only to define them, but also to articulate what values they encode and what their weaknesses are.\n",
    "\n",
    "1. In your own work, __you shouldn't feel confined to these metrics__. Per item 1 above, you should feel that you have the freedom to motivate new metrics and specific uses of existing metrics, depending on what your goals are.\n",
    "\n",
    "1. If you're working on an established problem, then you'll feel pressure from readers (and referees) to use the metrics that have already been used for the problem. This might be a compelling pressure. However, you should always feel free to argue against those cultural norms and motivate new ones. Areas can stagnate due to poor metrics, so we must be vigilant!\n",
    "\n",
    "This notebook discusses prominent metrics in NLP evaluations. I've had to be selective to keep the notebook from growing too long and complex. I think the measures and considerations here are fairly representative of the issues that arise in NLP evaluation.\n",
    "\n",
    "The scikit-learn [model evaluation usage guide](http://scikit-learn.org/stable/modules/model_evaluation.html) is excellent as a source of implementations, definitions, and references for a wide range of metrics for classification, regression, ranking, and clustering.\n",
    "\n",
    "This notebook is the first in a two-part series on evaluation. Part 2 is on [evaluation methods](evaluation_methods.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.translate import bleu_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "1. A confusion matrix gives a complete comparison of how the observed/gold labels compare to the labels predicted by a classifier.\n",
    "\n",
    "1. For classifiers that predict real values (scores, probabilities), it is important to remember that __a threshold was imposed to create these categorical predictions__. \n",
    "\n",
    "1. The position of this threshold can have a large impact on the overall assessment that uses the confusion matrix as an input. The default is to choose the class with the highest probability. This is so deeply ingrained that it is often not even mentioned. However, it might be inappropriate:\n",
    "  1. We might care about the full distribution.\n",
    "  1. Where the important class is very small relative to the others, any significant amount of positive probability for it might be important.\n",
    "\n",
    "1. Metrics like [average precision](#Average-precision) explore this threshold as part of their evaluation procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates the toy confusion matrices that we will use for illustrative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustrative_confusion_matrix(data):\n",
    "    classes = ['pos', 'neg', 'neutral']\n",
    "    ex = pd.DataFrame(\n",
    "        data,        \n",
    "        columns=classes,\n",
    "        index=classes)\n",
    "    ex.index.name = \"observed\"\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "[Accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) is the sum of the correct predictions divided by the sum of all predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(cm):\n",
    "    return cm.values.diagonal().sum() / cm.values.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an illustrative confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pos  neg  neutral\n",
       "observed                   \n",
       "pos        15   10      100\n",
       "neg        10   15       10\n",
       "neutral    10  100     1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1 = illustrative_confusion_matrix([\n",
    "    [15,  10,  100],\n",
    "    [10,  15,   10],\n",
    "    [10, 100, 1000]])\n",
    "\n",
    "ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110236220472441"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best.\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "Accuracy seems to directly encode a core value we have for classifiers – how often they are correct. In addition, the accuracy of a classifier on a test set will be negatively correlated with the [negative log (logistic, cross-entropy) loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss), which is a common loss for classifiers. In this sense, these classifiers are optimizing for accuracy.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "* Accuracy does not give per-class metrics for multi-class problems.\n",
    "\n",
    "* Accuracy fails to control for size imbalances in the classes. For instance, consider the variant of the above in which the classifier guessed only __neutral__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pos  neg  neutral\n",
       "observed                   \n",
       "pos         0    0      125\n",
       "neg         0    0       35\n",
       "neutral     0    0     1110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex2 = illustrative_confusion_matrix([\n",
    "    [0, 0,  125],\n",
    "    [0, 0,   35], \n",
    "    [0, 0, 1110]])\n",
    "\n",
    "ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, this is a worse classifier than the one that produced `ex1`. Whereas `ex1` does well at __pos__ and __neg__ despite their small size, this classifier doesn't even try to get them right – it always predicts __neutral__. However, its accuracy is higher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8740157480314961"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(ex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related\n",
    "\n",
    "* Accuracy is closely related to the [negative log (logistic, cross-entropy) loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss) metric, as described above.\n",
    "\n",
    "* The \"one-hot\" vectors that constitute the labels for most classifiers can be seen as a special case of full probability distributions over the labels. If one's labels are probabilistic in this sense, and one's classifier predicts such distributions as well, then  [KL divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) is the natural generalization of the log-loss and can thus be seen as a counterpart to accuracy in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "[Precision](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) is the sum of the correct predictions divided by the sum of all guesses. This is a per-class notion; in our confusion matrices, it's the diagonal values divided by the column sums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(cm):\n",
    "    return cm.values.diagonal() / cm.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos        0.428571\n",
       "neg        0.120000\n",
       "neutral    0.900901\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our problematic __all neutral__ classifier above, precision is strictly speaking undefined for __pos__ and __neg__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos             NaN\n",
       "neg             NaN\n",
       "neutral    0.874016\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(ex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common to see these `NaN` values mapped to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best.\n",
    "\n",
    "#### Value encoded \n",
    "\n",
    "Precision encodes a _conservative_ value in penalizing incorrect guesses.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "Precision's dangerous edge case is that one can achieve very high precision for a category by rarely guessing it. Consider, for example, the following classifier's flawless predictions for __pos__ and __neg__. These predictions are at the expense of __neutral__, but that is such a big class that it hardly matters to the precision for that class either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pos  neg  neutral\n",
       "observed                   \n",
       "pos         1    0      124\n",
       "neg         0    1       24\n",
       "neutral     0    0     1110"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex3 = illustrative_confusion_matrix([\n",
    "    [1, 0,  124], \n",
    "    [0, 1,   24], \n",
    "    [0, 0, 1110]])\n",
    "\n",
    "ex3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos        1.000000\n",
       "neg        1.000000\n",
       "neutral    0.882353\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers mask the fact that this is a very poor classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "[Recall](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) is the sum of the correct predictions divided by the sum of all true instances. This is a per-class notion; in our confusion matrices, it's the diagonal values divided by the row sums. Recall is sometimes called the \"true positive rate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(cm):\n",
    "    return cm.values.diagonal() / cm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "observed\n",
       "pos        0.120000\n",
       "neg        0.428571\n",
       "neutral    0.900901\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall trades off against precision. For instance, consider again `ex3`, in which the classifier was very conservative with __pos__ and __neg__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos        1.000000\n",
       "neg        1.000000\n",
       "neutral    0.882353\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, recall is very low here because the classifier guessed __neutral__ for so many of these classes' true instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "observed\n",
       "pos        0.008\n",
       "neg        0.040\n",
       "neutral    1.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best.\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "Recall encodes a _permissive_ value in penalizing only missed true cases.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "Recall's dangerous edge case is that one can achieve very high recall for a category by always guessing it. This could mean a lot of incorrect guesses, but recall sees only the correct ones. You can see this in `ex3` above. The model did make some incorrect __neural__ predictions, but it missed none, so it achieved perfect recall for that category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F scores\n",
    "\n",
    "[F scores](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score) combine precision and recall via their harmonic mean, with a value $\\beta$ that can be used to emphasize one or the other. Like precision and recall, this is a per-category notion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(cm, beta):\n",
    "    p = precision(cm)\n",
    "    r = recall(cm)\n",
    "    return (beta**2 + 1) * ((p * r) / ((beta**2 * p) + r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `beta=1`, this is the __F1 score__, which gives equal weight to both categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(cm):\n",
    "    return f_score(cm, beta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos        0.187500\n",
       "neg        0.187500\n",
       "neutral    0.900901\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos             NaN\n",
       "neg             NaN\n",
       "neutral    0.932773\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos        0.015873\n",
       "neg        0.076923\n",
       "neutral    0.937500\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best, and guaranteed to be between precision and recall.\n",
    "\n",
    "#### Value encoded \n",
    "\n",
    "The F$_{\\beta}$ score for a class $K$ is an attempt to summarize how well the classifier's $K$ predictions align with the true instances of $K$. Alignment brings in both missed cases and incorrect predictions. Intuitively, precision and recall keep each other in check in the calculation. This idea runs through almost all robust classification metrics.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "* For a given category $K$, the F$_{\\beta}$ score for $K$ ignores  all the values that are off the row and column for $K$, which might be the majority of the data. This means that the individual scores for a category can be very misleading about the overall performance of the system. \n",
    "\n",
    "* There is no normalization for the size of the dataset within $K$ or outside of it.\n",
    "\n",
    "* We get a score per class. This can be a virtue, but it can also be an obstacle, especially if one is doing automatic hyperparameter selection and cross-validation, as those processes require a single score.\n",
    "\n",
    "#### Related\n",
    "\n",
    "* Dice similarity for binary vectors is sometimes used to assess how well a model has learned to identify a set of items. In this setting, [it is equivalent to the per-token F1 score](https://brenocon.com/blog/2012/04/f-scores-dice-and-jaccard-set-similarity/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro-averaged F scores\n",
    "\n",
    "The [macro-averaged F$_{\\beta}$ score](http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification) (macro F$_{\\beta}$) is the mean of the F$_{\\beta}$ score for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f_score(cm, beta):\n",
    "    return f_score(cm, beta).mean(skipna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42530030030030036"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f_score(ex1, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f_score(ex2, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34343203093203095"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f_score(ex3, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best, and guaranteed to be between precision and recall.\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "Macro F$_{\\beta}$ scores inherit the values of F$_{\\beta}$ scores, and they additionally say that we care about all the classes equally regardless of their size. \n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "In NLP, we typically care about modeling all of the classes well, so macro-F$_{\\beta}$ scores often seem appropriate. However, this is also the source of its primary weaknesses:\n",
    "\n",
    "* If a model is doing really well on a small class $K$, its high macro F$_{\\beta}$ score might mask the fact that it mostly makes incorrect predictions outside of $K$. So F$_{\\beta}$ scoring will make this kind of classifier look better than it is.\n",
    "\n",
    "* Conversely, if a model does well on a very large class, its overall performance might be high even if it stumbles on some small classes. So F$_{\\beta}$ scoring will make this kind of classifier look worse than it is, as measures by sheer number of good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted F scores\n",
    "\n",
    "[Weighted F$_{\\beta}$ scores](http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification) average the per-category F$_{\\beta}$ scores, but it's a weighted average based on the size of the classes in the observed/gold data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_f_score(cm, beta):\n",
    "    scores = f_score(cm, beta=beta).values\n",
    "    weights = cm.sum(axis=1)\n",
    "    return np.average(scores, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828993812624765"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_f_score(ex3, beta=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best, but without a guarantee that it will be between precision and recall.\n",
    "\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "Weighted F$_{\\beta}$ scores inherit the values of F$_{\\beta}$ scores, and they additionally say that we want to weight the summary by the number of actual and predicted examples in each class. This will probably correspond well with how the classifier will perform, on a per example basis, on data with the same class distribution as the training data.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "Large classes will dominate these calculations. Just like macro-averaging, this can make a classifier look artificially good or bad, depending on where its errors tend to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-averaged F scores\n",
    "\n",
    "[Micro-averaged F$_{\\beta}$ scores](http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification) (micro F$_{\\beta}$ scores) add up the 2 $\\times$ 2 confusion matrices for each category versus the rest, and then they calculate the F$_{\\beta}$ scores, with the convention being that the positive class's F$_{\\beta}$ score is reported. \n",
    "\n",
    "For F1, this value is identical to both precision and recall on that 2 $\\times$ 2 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates the 2 $\\times$ 2 matrix for a category `cat` in a confusion matrix `cm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_versus_rest(cm, cat):\n",
    "    yes = cm.loc[cat, cat]\n",
    "    yes_no = cm[cat].sum() - yes\n",
    "    no_yes = cm.loc[cat].sum() - yes\n",
    "    no = cm.values.sum() - yes - yes_no - no_yes\n",
    "    return pd.DataFrame(\n",
    "        [[yes,    yes_no], \n",
    "         [no_yes,    no]],\n",
    "        columns=['yes', 'no'], \n",
    "        index=['yes', 'no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yes</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>110</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     yes    no\n",
       "yes   15    20\n",
       "no   110  1125"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_versus_rest(ex1, 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yes</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>15</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>20</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     yes    no\n",
       "yes   15   110\n",
       "no    20  1125"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_versus_rest(ex1, 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yes</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>1000</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      yes   no\n",
       "yes  1000  110\n",
       "no    110   50"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_versus_rest(ex1, 'neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the micro F$_{\\beta}$ score, we just add up these per-category confusion matrices and calculate the F$_{\\beta}$ score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f_score(cm, beta):\n",
    "    c = sum([cat_versus_rest(cm, cat) for cat in cm.index])\n",
    "    return f_score(c, beta=beta).loc['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110236220472442"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_f_score(ex1, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-class problems, this has an intuitive interpretation in which precision and recall are defined in terms of correct and incorrect guesses ignoring the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best, and guaranteed to be between precision and recall.\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "Micro F$_{\\beta}$ scores inherit the values of weighted F$_{\\beta}$ scores. (The resulting scores tend to be very similar.)\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "The weaknesses too are the same as those of weighted F$_{\\beta}$ scores, with the additional drawback that we actually get two potentially very different values, for the positive and negative classes, and we have to choose one to meet our goal of having a single summary number. (See the `'yes'` in the final line of `micro_f_score`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision–recall curves\n",
    "\n",
    "I noted above that confusion matrices hide a threshold for turning probabilities/scores into predicted labels. With precision–recall curves, we finally address this.\n",
    "\n",
    "A precision–recall curve is a method for summarizing the relationship between precision and recall for a binary classifier. \n",
    "\n",
    "The basis for this calculation is not the confusion matrix, but rather the raw scores or probabilities returned by the classifier. Normally, we use 0.5 as the threshold for saying that a prediction is positive. However, each distinct real value in the set of predictions is a potential threshold. The precision–recall curve explores this space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic implementation; [the sklearn version](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) is more flexible and so recommended for real experimental frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(y, probs):\n",
    "    \"\"\"`y` is a list of labels, and `probs` is a list of predicted\n",
    "    probabilities or predicted scores -- likely a column of the \n",
    "    output of `predict_proba` using an `sklearn` classifier.\n",
    "    \"\"\"\n",
    "    thresholds = sorted(set(probs))\n",
    "    data = []\n",
    "    for t in thresholds:\n",
    "        # Use `t` to create labels:\n",
    "        pred = [1 if p >= t else 0 for p in probs]\n",
    "        # Precision/recall analysis as usual, focused on\n",
    "        # the positive class:\n",
    "        cm = pd.DataFrame(metrics.confusion_matrix(y, pred))\n",
    "        prec = precision(cm)[1]\n",
    "        rec = recall(cm)[1]\n",
    "        data.append((t, prec, rec))\n",
    "    # For intuitive graphs, always include this end-point:\n",
    "    data.append((None, 1, 0))\n",
    "    return pd.DataFrame(\n",
    "        data, columns=['threshold', 'precision', 'recall'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what precision–recall curves look like, let's use `sklearn`'s `make_classification` function to to create an artificial classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(\n",
    "    class_sep=0.6, n_samples=1000, n_features=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making `class_sep` bigger will make the task easier, and changing `n_features` (and/or its related values `n_informative`, `n_redundant`, and `n_repeated`) will change how much information is available for predicting the labels.\n",
    "\n",
    "With this dataset, we create a random train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a classifier as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = LogisticRegression()\n",
    "_ = mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the classifier to make predictions; the second step here keeps just the probabilities for the positive class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mod.predict_proba(X_test)\n",
    "\n",
    "_, pos_predictions = zip(*predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc = precision_recall_curve(y_test, pos_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0b4d6d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = prc.plot(x='recall', y='precision')\n",
    "ax.set_xlim([0, 1])\n",
    "_ = ax.set_ylim([0, 1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value encoded\n",
    "\n",
    "With precision–recall curves, we get a generalized perspective on F1 scores (and we could weight precision and recall differently to achieve the effects of `beta` for F scores more generally). These curves can be used, not only to assess a system, but also to identify an optimal decision boundary given external goals.    \n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "* Most implementations are limited to binary problems. The basic concepts are defined for multi-class problems, but it's very difficult to understand the resulting hyperplanes.\n",
    "\n",
    "* There is no single statistic that does justice to the full curve, so this metric isn't useful on its own for guiding development and optimization. Indeed, opening up the decision threshold in this way really creates another hyperparameter that one has to worry about!\n",
    "\n",
    "#### Related\n",
    "\n",
    "* The [Receive operating characteristic (ROC) curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) is very similar to the precision–recall curve. However, instead of balancing precision and recall, it balances recall against the false positive rate. Precision–recall curves are more appropriate than ROC curves in situations where the two classes are very different in size. Since this is common in NLP, precision–recall curves are more common.\n",
    "\n",
    "* [Average precision](#Average-precision), covered next, is a way of summarizing these curves with a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average precision\n",
    "\n",
    "Average precision is a method for summarizing the precision–recall curve. It does this by calculating the average precision weighted by the change in recall from step to step along the curve. Here is the calculation in terms of the data structures returned by `precision_recall_curve` above, in which (as in sklearn) the largest recall value is first:\n",
    "\n",
    "$$\\textbf{average-precision}(r, p) = \\sum_{i=1}^{n} (r_{n} - r_{n+1})p_{n}$$\n",
    "\n",
    "where $n$ is the increasing sequence of thresholds and the precision and recall vectors $p$ and $r$ are of length $n+1$. (We insert a final pair of values $p=1$ and $r=0$ in the precision–recall curve calculation, with no threshold for that point.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(p, r):\n",
    "    total = 0.0\n",
    "    for i in range(len(p)-1):\n",
    "        total += (r[i] - r[i+1]) * p[i]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.933476153332317"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision(prc['precision'].values, prc['recall'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best.\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "This measure is very similar to the F1 score, in that it is seeking to balance precision and recall. Whereas the F1 score does this with the harmonic mean, average precision does it by making precision a function of recall.\n",
    "    \n",
    "#### Weaknesses\n",
    "\n",
    "* An important weakness of this metric is cultural: it is often hard to tell whether a paper is reporting average precision or some interpolated variant thereof. The interpolated versions are meaningfully different and will tend to inflate scores. In any case, they are not comparable to the calculation defined above and implemented in `sklearn` as `sklearn.metrics.average_precision_score`.\n",
    "\n",
    "* Unlike for precision–recall curves, we aren't strictly speaking limited to binary classification here. Since we aren't trying to visualize anything, we can do these calculations for multi-class problems. However, then we have to decide on how the precision and recall values will be combined for each step: macro-averaged, weighted, or micro-averaged, just as with F$_{\\beta}$ scores. This introduces another meaningful design choice.\n",
    "\n",
    "#### Related\n",
    "\n",
    "* There are interpolated versions of this score, and some tasks/communities have even settled on specific versions as their standard metrics. All such measures should be approached with skepticism, since all of them can inflate scores artificially in specific cases. \n",
    "\n",
    "* [This blog post](https://roamanalytics.com/2016/09/07/stepping-away-from-linear-interpolation/) is an excellent discussion of the issues with linear interpolation. It proposes a step-wise interpolation procedure that is much less problematic. I believe the blog post and subsequent PR to `sklearn` led the `sklearn` developers to drop support for all interpolation mechanisms for this metric!\n",
    "\n",
    "* Average precision as defined above is a discrete approximation of the [area under the precision–recall curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#sklearn.metrics.auc). This is a separate measure often referred to as \"AUC\". In calculating AUC for a precision–recall curve, some kind of interpolation will be done, and this will generally produce exaggerated scores for the same reasons that interpolated average precison does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error\n",
    "\n",
    "The [mean squared error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) is a summary of the distance between predicted and actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    diffs = (y_true - y_pred)**2\n",
    "    return np.mean(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw distances `y_true - y_pred` are often called the __residuals__.\n",
    "\n",
    "#### Bounds\n",
    "\n",
    "[0, $\\infty$), with 0 the best.\n",
    "\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "This measure seeks to summarize the errors made by a regression classifier. The smaller it is, the closer the model's predictions are to the truth. In this sense, it is intuitively like a counterpart to [accuracy](#Accuracy) for classifiers.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "These values are highly dependent on scale of the output variables, making them very hard to interpret in isolation. One really needs a clear baseline, and scale-independent ways of comparing scores are also needed.\n",
    "\n",
    "#### Related\n",
    "\n",
    "Scikit-learn implements a variety of closely related measures: [mean absolute error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error), [mean squared logarithmic error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error), and [median absolute error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error). I'd say that one should choose among these metrics based on how the output values are scaled and distributed. For instance, the median absolute error will be less sensitive to outliers than the others, and mean squared logarithmic error might be more appropriate where the outputs are not strictly speaking linearly increasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 scores\n",
    "\n",
    "The R$^{2}$ score is probably the most prominent method for summarizing regression model performance, in statistics, social sciences, and ML/NLP. This is the value that `sklearn`'s regression models deliver with their `score` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    mu = y_true.mean()\n",
    "    # Total sum of squares:\n",
    "    total = ((y_true - mu)**2).sum()\n",
    "    # Sum of squared errors:\n",
    "    res = ((y_true - y_pred)**2).sum()    \n",
    "    return 1.0 - (res / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds\n",
    "\n",
    "[0, 1], with 0 the worst and 1 the best.\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "The numerator in the R$^{2}$ calculation is the sum of errors. In the context of regular linear regression, the model's objective is to minimize the total sum of squares, which is the denominator in the calculation. Thus, R$^{2}$ is based in the ratio between what the model achieved and what its objective was, which is a measure of the goodness of fit of the model.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "For comparative purposes, it's nice that R$^{2}$ is scaled between [0, 1]; as noted above, this lack of scaling makes mean squared error hard to interpret. But this also represents a trade-off: R$^{2}$ doesn't tell us about the magnitude of the errors.\n",
    "\n",
    "\n",
    "#### Related\n",
    "\n",
    "* R$^{2}$ is [closely related to the squared Pearson correlation coefficient](https://en.wikipedia.org/wiki/Coefficient_of_determination#As_squared_correlation_coefficient).\n",
    "\n",
    "* R$^{2}$ is closely related to the [explained variance](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score), which is also defined in terms of a ratio of the residuals and the variation in the gold data. For explained variance, the numerator is the variance of the residuals and the denominator is the variance of the gold values.\n",
    "\n",
    "* [Adjusted R$^{2}$](https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2) seeks to take into account the number of predictors in the model, to reduce the incentive to simply add more features in the hope of lucking into a better score. In ML/NLP, relatively little attention is paid to model complexity in this sense. The attitude is like: if you can improve your model by adding features, you might as well do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence prediction\n",
    "\n",
    "Sequence prediction metrics all seek to summarize and quantify the extent to which a model has managed to reproduce, or accurately match, some gold standard sequences. Such problems arise throughout NLP. Examples: \n",
    "\n",
    "1. Mapping speech signals to their desired transcriptions.\n",
    "1. Mapping texts in a language $L_{1}$ to their translations in a distinct language or dialect $L_{2}$.\n",
    "1. Mapping input dialogue acts to their desired responses.\n",
    "1. Mapping a sentence to one of its paraphrases.\n",
    "1. Mapping real-world scenes or contexts (non-linguistic) to descriptions of them (linguistic).\n",
    "\n",
    "Evaluations is very challenging because the relationships tend to be __many-to-one__: a given sentence might have multiple suitable translations; a given dialogue act will always have numerous felicitous responses; any scene can be described in multiple ways; and so forth. The most constrained of these problems is the speech-to-text case in 1, but even that one has indeterminacy in real-world contexts (humans often disagree about how to transcribe spoken language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word error rate\n",
    "\n",
    "The [word error rate](https://en.wikipedia.org/wiki/Word_error_rate) (WER) metric is a word-level, length-normalized measure of [Levenshtein string-edit distance](https://en.wikipedia.org/wiki/Levenshtein_distance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(seq_true, seq_pred):\n",
    "    d = edit_distance(seq_true, seq_pred)\n",
    "    return d / len(seq_true)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(['A', 'B', 'C'], ['A', 'A', 'C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(['A', 'B', 'C', 'D'], ['A', 'A', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate this over the entire test-set, one gets the edit-distances for each gold–predicted pair and normalizes these by the length of all the gold examples, rather than normalizing each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_wer(y_true, y_pred):\n",
    "    dists = [edit_distance(seq_true, seq_pred) \n",
    "             for seq_true, seq_pred in zip(y_true, y_pred)]\n",
    "    lengths = [len(seq) for seq in y_true]\n",
    "    return sum(dists) / sum(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a single summary value for the entire set of errors.\n",
    "\n",
    "#### Bounds\n",
    "\n",
    "$[0, \\infty)$, where 0 is best. (The lack of a finite upper bound derives from the fact that the normalizing constant is given by the true sequences, and the predicted sequences can differ from them in any conceivable way in principle.)\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "This method says that our desired notion of closeness or accuracy can be operationalized in terms of the low-level operations of insertion, deletion, and substitution. The guiding intuition is very much like that of F scores.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "The value encoded reveals a potential weakness in certain domains. Roughly, the more __semantic__ the task, the less appropriate WER is likely to be. For example, adding a negation to a sentence will radically change its meaning but incur only a small WER penalty, whereas passivizing a sentence (_Kim won the race_ &rarr; _The race was won by Kim_) will hardly change its meaning at all but incur a large WER penalty. See also [Liu et al. 2016](https://www.aclweb.org/anthology/D16-1230) for similar arguments in the context of dialogue generation.\n",
    "\n",
    "#### Related\n",
    "\n",
    "* WER can be thought of as a family of different metrics varying in the notion of edit distance that they employ.\n",
    "\n",
    "* The Word Accuracy Rate is 1.0 minus the WER, which, despits its name, is intuitively more like [recall](#Recall) than [accuracy](#Accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU scores\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) scores were originally developed in the context of machine translation, but they are applied in other generation tasks as well. For BLEU scoring, we require a set of gold outputs. The metric has two main components:\n",
    "\n",
    "* __Modified n-gram precision__: A direct application of precision would divide the number of correct n-grams in the predicted output (n-grams that appear in any translation) by the number of n-grams in the predicted output. This has a degenerate solution in which the predicted output contains only one word. BLEU's modified version substitutes the actual count for each n-gram by the maximum number of times it appears in any translation.\n",
    "\n",
    "* __Brevity penalty (BP)__: to avoid favoring outputs that are too short, a penalty is applied. Let $Y$ be the set of gold outputs, $\\widetilde{y}$ the predicted output, $c$ the length of the predicted output, and $r$ the smallest absolute difference between the length of $c$ and the length of any of its gold outputs in $Y$. Then:\n",
    "\n",
    "$$\\textbf{BP}(Y, \\widetilde{y}) =\n",
    "\\begin{cases}\n",
    "1 & \\textrm{ if } c > r \\\\\n",
    "\\exp(1 - \\frac{r}{c}) & \\textrm{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "The BLEU score itself is typically a combination of modified n-gram precision for various $n$ (usually up to 4):\n",
    "\n",
    "$$\\textbf{BLEU}(Y, \\widetilde{y}) = \\textbf{BP}(Y, \\widetilde{y}) \\cdot \n",
    "    \\exp\\left(\\sum_{n=1}^{N} w_{n} \\cdot \\log\\left(\\textbf{modified-precision}(Y, \\widetilde{y}, n\\right)\\right)$$\n",
    "\n",
    "where $Y$ is the set of gold outputs, $\\widetilde{y}$ is the predicted output, and $w_{n}$ is a weight for each $n$-gram level (usually set to $1/N$).\n",
    "\n",
    "NLTK has [implementations of Bleu scoring](http://www.nltk.org/_modules/nltk/translate/bleu_score.html) for the sentence-level, as defined above, and for the corpus level (`nltk.translate.bleu_score.corpus_bleu`). At the corpus level, it is typical to do a kind of [micro-averaging](#Micro-averaged-F-scores) of the modified precision scores and use a cumulative version of the brevity penalty.\n",
    "\n",
    "#### Bounds\n",
    "\n",
    "[0, 1], with 1 being the best, though with no expectation that any system will achieve 1, since even sets of human-created translations do not reach this level.\n",
    "\n",
    "#### Value encoded\n",
    "\n",
    "BLEU scores attempt to achieve the same balance between precision and recall that runs through the majority of the metrics discussed here. It has many affinities with [word error rate](#Word-error-rate), but seeks to accommodate the fact that there are typically multiple suitable outputs for a given input.\n",
    "\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "* [Callison-Burch et al. (2006)](http://www.aclweb.org/anthology/E06-1032) criticize BLEU as a machine translation metric on the grounds that it fails to correlate with human scoring of translations. They highlight its insensitivity  to n-gram order and its insensitivity to n-gram types (e.g., function vs. content words) as causes of this lack of correlation.\n",
    "\n",
    "* [Liu et al. (2016)](https://www.aclweb.org/anthology/D16-1230) specifically argue against BLEU as a metric for assessing dialogue systems, based on a lack of correlation with human judgments about dialogue coherence.\n",
    "\n",
    "#### Related\n",
    "\n",
    "There are many competitors/alternatives to BLEU, most proposed in the context of machine translation. Examples: [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric), [METEOR](https://en.wikipedia.org/wiki/METEOR), [HyTER](http://www.aclweb.org/anthology/N12-1017), [Orange (smoothed Bleu)](http://www.aclweb.org/anthology/C04-1072)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "[Perplexity](https://en.wikipedia.org/wiki/Perplexity) is a common metric for directly assessing generation models by calculating the probability that they assign to sequences in the test data. It is based in a measure of average surprisal:\n",
    "\n",
    "$$H(P, x) = -\\frac{1}{m}\\log_{2} P(x)$$\n",
    "\n",
    "where $P$ is a model assigning probabilities to sequences and $x$ is a sequence.\n",
    "\n",
    "Perplexity is then the exponent of this:\n",
    "\n",
    "$$\\textbf{perplexity}(P, x) = 2^{H(P, x)}$$\n",
    "\n",
    "Using any base $n$ both in defining $H$ and as the base in $\\textbf{perplexity}$ will lead to identical results.\n",
    "\n",
    "Minimizing perplexity is equivalent to maximizing probability.\n",
    "\n",
    "It is common to report per-token perplexity; here the averaging should be done in log-space to deliver a [geometric mean](https://en.wikipedia.org/wiki/Geometric_mean):\n",
    "\n",
    "$$\\textbf{token-perplexity}(P, x) = \\exp\\left(\\frac{\\log\\textbf{perplexity}(P, x)}{\\textbf{length}(x)}\\right)$$\n",
    "\n",
    "\n",
    "When averaging perplexity values obtained from all the sequences in a text corpus, one should again use the geometric mean:\n",
    "\n",
    "$$\\textbf{mean-perplexity}(P, X) = \n",
    "\\exp\\left(\\frac{1}{m}\\sum_{x\\in X}\\log(\\textbf{token-perplexity}(P, x))\\right)$$\n",
    "\n",
    "for a set of $m$ examples $X$.\n",
    "\n",
    "#### Bounds\n",
    "\n",
    "[1, $\\infty$], where 1 is best.\n",
    "\n",
    "#### Values encoded\n",
    "\n",
    "The guiding idea behind perplexity is that a good model will assign high probability to the sequences in the test data. This is an intuitive, expedient intrinsic evaluation, and it matches well with the objective for models trained with a cross-entropy or logistic objective.\n",
    "\n",
    "#### Weaknesses\n",
    "\n",
    "* Perplexity is heavily dependent on the nature of the underlying vocabulary in the following sense: one can artificially lower one's perplexity by having a lot of `UNK` tokens in the training and test sets. Consider the extreme case in which _everything_ is mapped to `UNK` and perplexity is thus perfect on any test set. The more worrisome thing is that any amount of `UNK` usage side-steps the pervasive challenge of dealing with infrequent words.\n",
    "\n",
    "* [As Hal Daumé discusses in this post](https://nlpers.blogspot.com/2014/05/perplexity-versus-error-rate-for.html), the perplexity metric imposes an artificial constrain that one's model outputs are probabilistic.\n",
    "\n",
    "#### Related\n",
    "\n",
    "Perplexity is the inverse of probability and, [with some assumptions](http://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/gauntlet.pdf), can be seen as an approximation of the cross-entropy between the model's predictions and the true underlying sequence probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
